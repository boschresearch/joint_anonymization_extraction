{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage of the Stacked model for joint anonymization (ANON) and concept extraction (CE)\n",
    "\n",
    "Copyright (c) 2020 Robert Bosch GmbH\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU Affero General Public License as published\n",
    "by the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU Affero General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Affero General Public License\n",
    "along with this program.  If not, see <https://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'spanish_stacked'\n",
    "# Name of the experiments and also the storage path\n",
    "# This could also be a nested directory, e.g., 'test/v1/spanish_stacked/'\n",
    "# This path will be created if non-existent\n",
    "print(f'Storing at: {experiment}/')\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "# Set the learning rates for ANON and CE trainers\n",
    "# A slightly higher learning rate is used for the main task CE\n",
    "learning_rate_anon = 0.1\n",
    "learning_rate_ce = 0.2\n",
    "\n",
    "# The STACKED model is trained on anonymization data for a couple \n",
    "# of epochs before the actual multitask training starts. \n",
    "# This gives the model good anonymization performance before \n",
    "# the concept extraction uses the anonymziation internally.\n",
    "pretrain_epochs_anon = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anon_corpus and ce_corpus can be any corpora\n",
    "# inheriting from flair.data.ColumnCorpus\n",
    "#\n",
    "# Check out the following link for more information:\n",
    "# https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md\n",
    "#\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "# Get the corpus for concept extraction (CE)\n",
    "# Adapt the paths and filenmes to your setting\n",
    "print('Load CE data')\n",
    "ce_tag_type = 'ce'\n",
    "ce_columns = {0: 'text', 1: ce_tag_type}\n",
    "\n",
    "ce_corpus = ColumnCorpus(\n",
    "    'data/pharmaconer/', ce_columns, \n",
    "    tag_to_bioes=ce_tag_type, # is used to convert BIO to BIOES labels\n",
    "    train_file='train.sample.bio', # NOTE that these files are samples\n",
    "    dev_file='dev.sample.bio',     # and not the complete dataset\n",
    "    test_file='test.sample.bio')   # which we do not ship with this repo\n",
    "print(ce_corpus)\n",
    "\n",
    "# make the tag dictionary from the corpus\n",
    "ce_tag_dictionary = ce_corpus.make_tag_dictionary(tag_type=ce_tag_type)\n",
    "print(f'found {len(ce_tag_dictionary)} labels')\n",
    "print(ce_tag_dictionary.idx2item)\n",
    "\n",
    "\n",
    "# Now the same for anonymization (ANON) data\n",
    "# Adapt the paths and filenmes to your setting\n",
    "anon_tag_type = 'anon'\n",
    "anon_columns = {0: 'text', 1: anon_tag_type}\n",
    "\n",
    "print('Load ANON data')\n",
    "anon_corpus = ColumnCorpus(\n",
    "    'data/meddocan/', anon_columns,\n",
    "    train_file='train.sample.bio', # NOTE that these files are samples\n",
    "    dev_file='dev.sample.bio',     # and not the complete dataset\n",
    "    test_file='test.sample.bio')   # which we do not ship with this repo\n",
    "print(anon_corpus)\n",
    "\n",
    "# make the tag dictionary from the corpus\n",
    "anon_tag_dictionary = anon_corpus.make_tag_dictionary(tag_type=anon_tag_type)\n",
    "print(f'found {len(anon_tag_dictionary)} labels')\n",
    "print(anon_tag_dictionary.idx2item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the embeddings you want to use in your experiments\n",
    "#\n",
    "# Check out the following link for more information:\n",
    "# https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md\n",
    "#\n",
    "from flair.embeddings import * \n",
    "from src.embeddings import CustomBytePairEmbeddings\n",
    "\n",
    "# Embedding can be any (combination of) embedding(s) \n",
    "# inheriting from flair.embeddings.TokenEmbeddings.\n",
    "# We use the concatenation of the following embeddings\n",
    "embeddings: TokenEmbeddings = StackedEmbeddings([\n",
    "    WordEmbeddings('cc.es.300.vec.gensim'),            # * Spanish FastText converted to gensim format\n",
    "    WordEmbeddings('Scielo_wiki_Fasttext.vec.gensim'), # * Domain-specific fastText (https://www.aclweb.org/anthology/W19-1916.pdf)\n",
    "    CustomBytePairEmbeddings(language='es'),           # * Spanish BPEmb\n",
    "    BertEmbeddings('bert-base-multilingual-uncased',   # * multilingual BERT embeddings\n",
    "                   layers='-1,-2,-3,-4',               #   using a scalar mix of the last layers\n",
    "                   use_scalar_mix=True),               # \n",
    "    FlairEmbeddings('es-forward'),                     # * Flair forward language model\n",
    "    FlairEmbeddings('es-backward')                     # * Flair backward language model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import StackedSequenceTagger # for STACKED\n",
    "#from src.models import MultitaskSequenceTagger # for MULTITASK\n",
    "#\n",
    "# Both models follow the same API. Changing the import and the initialization (line 8)\n",
    "# is enough to train a MultiTask model instead\n",
    "\n",
    "# Create stacked model for joint ANON and CE\n",
    "tagger = StackedSequenceTagger(\n",
    "    hidden_size = 256,\n",
    "    embeddings = embeddings,\n",
    "    ce_tag_dictionary = ce_tag_dictionary,\n",
    "    anon_tag_dictionary = anon_tag_dictionary,\n",
    "    ce_tag_type = ce_tag_type,\n",
    "    anon_tag_type = anon_tag_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainers import BatchWiseMultiTaskTrainer, SingleTaskTrainer\n",
    "\n",
    "# Create ANON trainer\n",
    "anon_trainer = SingleTaskTrainer(\n",
    "    tagger, anon_corpus, \n",
    "    model_mode='ANON'\n",
    ")\n",
    "\n",
    "# Create CE trainer\n",
    "ce_trainer = SingleTaskTrainer(\n",
    "    tagger, ce_corpus, \n",
    "    model_mode='CE'\n",
    ")\n",
    "\n",
    "# Create MultitaskTrainer\n",
    "mt_trainer = BatchWiseMultiTaskTrainer(\n",
    "    trainer_list = [anon_trainer, ce_trainer], \n",
    "    base_path_list = [\n",
    "        f'{experiment}/{anon_trainer.display_name}', \n",
    "        f'{experiment}/{ce_trainer.display_name}'], \n",
    "    learning_rate_list = [learning_rate_anon, learning_rate_ce],\n",
    "    pretrain_list = [pretrain_epochs_anon, 0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training for <epochs> epochs\n",
    "mt_trainer.train(\n",
    "    max_epochs=epochs,\n",
    "    mini_batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model according to CE development score\n",
    "try:\n",
    "    tagger = StackedSequenceTagger.load(f'{experiment}/{ce_trainer.display_name}/best-model.pt')\n",
    "except ModuleNotFoundError: # when MultiTaskTrainer was trained instead\n",
    "    tagger = MultitaskSequenceTagger.load(f'{experiment}/{ce_trainer.display_name}/best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "# Get labels for a new sentence\n",
    "sent = Sentence('Paciente de 70 años de edad fue remitido al departamento '\\\n",
    "                'de Oncología iniciando tratamiento quimioterápico '\\\n",
    "                'adyuvante con Cisplatino y Gemcitabina .')\n",
    "\n",
    "# Predict ANON labels \n",
    "# ANON labels are in BIO format\n",
    "tagger.set_output_to_anon()\n",
    "tagger.predict(sent)\n",
    "\n",
    "# Predict CE labels\n",
    "# CE labels are in BIOES format\n",
    "tagger.set_output_to_ce()\n",
    "tagger.predict(sent)\n",
    "\n",
    "for token in sent:\n",
    "    text = token.text\n",
    "    anon_label = token.get_tag(anon_tag_type).value\n",
    "    ce_label = token.get_tag(ce_tag_type).value\n",
    "    print((text, anon_label, ce_label))\n",
    "    \n",
    "# Output could look like:\n",
    "# ('Paciente', 'O', 'O')\n",
    "# ('de', 'O', 'O')\n",
    "# ('70', 'B-EDAD_SUJETO_ASISTENCIA', 'O')\n",
    "# ('años', 'I-EDAD_SUJETO_ASISTENCIA', 'O')\n",
    "# ('de', 'O', 'O')\n",
    "# ('edad', 'O', 'O')\n",
    "# ('fue', 'O', 'O')\n",
    "# ('remitido', 'O', 'O')\n",
    "# ('al', 'O', 'O')\n",
    "# ('departamento', 'O', 'O')\n",
    "# ('de', 'O', 'O')\n",
    "# ('Oncología', 'O', 'O')\n",
    "# ('iniciando', 'O', 'O')\n",
    "# ('tratamiento', 'O', 'O')\n",
    "# ('quimioterápico', 'O', 'O')\n",
    "# ('adyuvante', 'O', 'O')\n",
    "# ('con', 'O', 'O')\n",
    "# ('Cisplatino', 'O', 'S-NORMALIZABLES')\n",
    "# ('y', 'O', 'O')\n",
    "# ('Gemcitabina', 'O', 'S-NORMALIZABLES')\n",
    "# ('.', 'O', 'O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "Convert your data to the right format and use the following evaluation scripts:\n",
    "\n",
    "* [PharmaCoNER](https://github.com/PlanTL-SANIDAD/PharmaCoNER-CODALAB-Evaluation-Script) (Spanish CE)\n",
    "* [MEDDOCAN](https://github.com/PlanTL-SANIDAD/MEDDOCAN-Evaluation-Script) (Spanish ANON)\n",
    "* [i2b2](https://github.com/EmilyAlsentzer/clinicalBERT/tree/master/downstream_tasks/ner_eval) (English)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
